"""Exploitability evaluator for game-theoretic assessment.

Measures how exploitable an agent's strategy is by computing
the payoff gap between their empirical strategy and the best
response to opponents' play.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any

from atp.evaluators.base import EvalCheck, EvalResult, Evaluator
from atp.loader.models import Assertion, TestDefinition
from atp.protocol import ATPEvent, ATPResponse
from game_envs.analysis.exploitability import (
    EmpiricalStrategy,
    ExploitabilityResult,
    compute_exploitability,
)
from game_envs.core.state import RoundResult

from atp_games.models import GameResult


@dataclass(frozen=True)
class ExploitabilityConfig:
    """Configuration for ExploitabilityEvaluator.

    Attributes:
        epsilon: Maximum exploitability threshold for
            pass. Default 0.15 per design spec.
        payoff_matrix_1: Payoff matrix for player 1
            (row player). Required for computation.
        payoff_matrix_2: Payoff matrix for player 2
            (column player). Required for computation.
        action_names_1: Action labels for player 1.
        action_names_2: Action labels for player 2.
    """

    epsilon: float = 0.15
    payoff_matrix_1: list[list[float]] | None = None
    payoff_matrix_2: list[list[float]] | None = None
    action_names_1: list[str] | None = None
    action_names_2: list[str] | None = None


class ExploitabilityEvaluator(Evaluator):
    """Evaluates strategy exploitability.

    For each player:
    1. Extracts empirical strategy from game history
    2. Computes best response to opponents' strategy
    3. Reports exploitability = BR payoff - current payoff

    A Nash equilibrium strategy has exploitability ~ 0.
    A dominated strategy (e.g. AllC in PD) has high
    exploitability.

    Compatible with ATP evaluator registry and scoring
    pipeline.
    """

    def __init__(
        self,
        config: ExploitabilityConfig | None = None,
    ) -> None:
        self._config = config or ExploitabilityConfig()

    @property
    def name(self) -> str:
        """Return evaluator name."""
        return "exploitability"

    async def evaluate(
        self,
        task: TestDefinition,
        response: ATPResponse,
        trace: list[ATPEvent],
        assertion: Assertion,
    ) -> EvalResult:
        """Evaluate via ATP evaluator interface.

        Extracts GameResult and payoff matrices from
        assertion config.
        """
        config = assertion.config
        game_result_data = config.get("game_result")
        if game_result_data is None:
            return self._create_result(
                [
                    self._create_check(
                        name="exploitability_data",
                        passed=False,
                        message="No game_result in assertion config",
                    ),
                ]
            )
        game_result = GameResult.from_dict(game_result_data)
        return self.evaluate_game(game_result, config)

    def evaluate_game(
        self,
        result: GameResult,
        config: dict[str, Any] | None = None,
    ) -> EvalResult:
        """Evaluate exploitability of a GameResult.

        Args:
            result: Completed game result with history.
            config: Config dict with keys: epsilon,
                payoff_matrix_1, payoff_matrix_2,
                action_names_1, action_names_2.

        Returns:
            EvalResult with exploitability checks.
        """
        eval_config = self._resolve_config(config)
        checks: list[EvalCheck] = []

        if not result.episodes:
            return self._create_result(
                [
                    self._create_check(
                        name="exploitability_data",
                        passed=False,
                        message="No episodes to evaluate",
                    ),
                ]
            )

        if eval_config.payoff_matrix_1 is None or eval_config.payoff_matrix_2 is None:
            return self._create_result(
                [
                    self._create_check(
                        name="exploitability_data",
                        passed=False,
                        message=(
                            "Payoff matrices required for exploitability computation"
                        ),
                    ),
                ]
            )

        # Extract empirical strategies from all episodes
        history = self._collect_history(result)
        players = list(result.average_payoffs.keys())

        if len(players) != 2:
            return self._create_result(
                [
                    self._create_check(
                        name="exploitability_data",
                        passed=False,
                        message=(
                            "Exploitability requires exactly "
                            f"2 players, got {len(players)}"
                        ),
                    ),
                ]
            )

        if not history:
            return self._create_result(
                [
                    self._create_check(
                        name="exploitability_data",
                        passed=False,
                        message="No action history available",
                    ),
                ]
            )

        p0, p1 = players[0], players[1]

        try:
            emp_0 = EmpiricalStrategy.from_history(history, p0)
            emp_1 = EmpiricalStrategy.from_history(history, p1)
        except ValueError as e:
            return self._create_result(
                [
                    self._create_check(
                        name="exploitability_data",
                        passed=False,
                        message=f"Cannot extract strategy: {e}",
                    ),
                ]
            )

        # Compute exploitability
        import numpy as np

        payoff_1 = np.array(eval_config.payoff_matrix_1)
        payoff_2 = np.array(eval_config.payoff_matrix_2)

        actions_1 = eval_config.action_names_1
        actions_2 = eval_config.action_names_2

        strat_1 = emp_0.to_array(
            actions_1 or [str(i) for i in range(payoff_1.shape[0])]
        )
        strat_2 = emp_1.to_array(
            actions_2 or [str(i) for i in range(payoff_1.shape[1])]
        )

        exploit_result = compute_exploitability(
            payoff_1,
            payoff_2,
            strat_1,
            strat_2,
            action_names_1=actions_1,
            action_names_2=actions_2,
        )

        # Check per-player exploitability
        checks.append(self._check_per_player(exploit_result, eval_config, p0, p1))

        # Check total exploitability
        checks.append(self._check_total(exploit_result, eval_config))

        # Empirical strategy summary
        checks.append(self._strategy_summary(emp_0, emp_1, p0, p1))

        return self._create_result(checks)

    def _resolve_config(
        self,
        override: dict[str, Any] | None,
    ) -> ExploitabilityConfig:
        """Merge override dict into default config."""
        if override is None:
            return self._config
        return ExploitabilityConfig(
            epsilon=override.get(
                "epsilon",
                self._config.epsilon,
            ),
            payoff_matrix_1=override.get(
                "payoff_matrix_1",
                self._config.payoff_matrix_1,
            ),
            payoff_matrix_2=override.get(
                "payoff_matrix_2",
                self._config.payoff_matrix_2,
            ),
            action_names_1=override.get(
                "action_names_1",
                self._config.action_names_1,
            ),
            action_names_2=override.get(
                "action_names_2",
                self._config.action_names_2,
            ),
        )

    def _collect_history(
        self,
        result: GameResult,
    ) -> list[RoundResult]:
        """Collect RoundResult objects from all episodes."""
        history: list[RoundResult] = []
        for ep in result.episodes:
            for step in ep.actions_log:
                if isinstance(step, dict) and "actions" in step:
                    history.append(
                        RoundResult(
                            round_number=step.get("round_number", len(history)),
                            actions=step["actions"],
                            payoffs=step.get("payoffs", {}),
                        )
                    )
                elif isinstance(step, dict):
                    # Actions log may store actions directly
                    history.append(
                        RoundResult(
                            round_number=len(history),
                            actions=step,
                            payoffs={},
                        )
                    )
        return history

    def _check_per_player(
        self,
        exploit: ExploitabilityResult,
        config: ExploitabilityConfig,
        p0: str,
        p1: str,
    ) -> EvalCheck:
        """Check per-player exploitability against epsilon."""
        details: dict[str, Any] = {
            "per_player": exploit.per_player,
            "best_responses": exploit.best_responses,
            "epsilon": config.epsilon,
        }

        failures: list[str] = []
        for pid, gap in exploit.per_player.items():
            if gap > config.epsilon:
                failures.append(f"{pid}: {gap:.4f}")

        if failures:
            return EvalCheck(
                name="per_player_exploitability",
                passed=False,
                score=max(
                    0.0,
                    1.0 - exploit.total / (2 * config.epsilon),
                ),
                message=(
                    f"Exploitable players "
                    f"(epsilon={config.epsilon}): " + ", ".join(failures)
                ),
                details=details,
            )

        return EvalCheck(
            name="per_player_exploitability",
            passed=True,
            score=max(
                0.0,
                1.0 - exploit.total / (2 * config.epsilon),
            ),
            message=(
                "All players within exploitability "
                f"threshold (epsilon={config.epsilon})"
            ),
            details=details,
        )

    def _check_total(
        self,
        exploit: ExploitabilityResult,
        config: ExploitabilityConfig,
    ) -> EvalCheck:
        """Check total exploitability."""
        # Total threshold is 2 * epsilon (sum of 2 players)
        total_threshold = 2 * config.epsilon
        passed = exploit.total <= total_threshold
        score = max(
            0.0,
            1.0 - exploit.total / total_threshold
            if total_threshold > 0
            else (1.0 if exploit.total == 0 else 0.0),
        )

        return EvalCheck(
            name="total_exploitability",
            passed=passed,
            score=min(1.0, score),
            message=(
                f"Total exploitability: {exploit.total:.4f}"
                f" (threshold: {total_threshold:.4f})"
            ),
            details={
                "total": exploit.total,
                "threshold": total_threshold,
            },
        )

    def _strategy_summary(
        self,
        emp_0: EmpiricalStrategy,
        emp_1: EmpiricalStrategy,
        p0: str,
        p1: str,
    ) -> EvalCheck:
        """Report empirical strategy summary."""
        return EvalCheck(
            name="empirical_strategy",
            passed=True,
            score=1.0,
            message="Empirical strategies extracted",
            details={
                p0: emp_0.to_dict(),
                p1: emp_1.to_dict(),
            },
        )
