#!/usr/bin/env python3
"""
ATP Task Executor ‚Äî automatic task execution via Claude CLI

Usage:
    python executor.py run                    # Execute next task
    python executor.py run --task=TASK-001    # Execute specific task
    python executor.py run --all              # Execute all ready tasks
    python executor.py run --milestone=mvp    # Execute milestone tasks
    python executor.py status                 # Execution status
    python executor.py retry TASK-001         # Retry failed task
    python executor.py logs TASK-001          # Task logs
"""

import argparse
import json
import re
import shutil
import subprocess
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path

import yaml

from task import (
    TASKS_FILE,
    Task,
    get_next_tasks,
    get_task_by_id,
    mark_all_checklist_done,
    parse_tasks,
    update_task_status,
)

# Configuration file path
CONFIG_FILE = Path("executor.config.yaml")


@dataclass
class ExecutorConfig:
    """Executor configuration"""

    max_retries: int = 3  # Max attempts per task
    retry_delay_seconds: int = 5  # Pause between attempts
    task_timeout_minutes: int = 30  # Task timeout
    max_consecutive_failures: int = 2  # Stop after N consecutive failures

    # Claude CLI
    claude_command: str = "claude"  # Claude CLI command
    claude_model: str = ""  # Model (empty = default)
    skip_permissions: bool = True  # Skip permission prompts

    # Hooks
    run_tests_on_done: bool = True  # Run tests on completion
    create_git_branch: bool = True  # Create branch on start
    auto_commit: bool = True  # Auto-commit on success

    # Paths
    project_root: Path = Path(".")
    logs_dir: Path = Path("spec/.executor-logs")
    state_file: Path = Path("spec/.executor-state.json")

    # Test command (using uv)
    test_command: str = "uv run pytest tests/ -v -m 'not slow'"
    lint_command: str = "uv run ruff check ."
    lint_fix_command: str = "uv run ruff check . --fix"  # Lint auto-fix command
    run_lint_on_done: bool = True  # Run lint on completion
    lint_blocking: bool = True  # Lint errors block task completion


def load_config_from_yaml(config_path: Path = CONFIG_FILE) -> dict:
    """Load configuration from YAML file.

    Args:
        config_path: Path to the configuration file.

    Returns:
        Dictionary with configuration values.
    """
    if not config_path.exists():
        return {}

    try:
        with open(config_path) as f:
            data = yaml.safe_load(f) or {}

        executor_config = data.get("executor", {})
        hooks = executor_config.get("hooks", {})
        pre_start = hooks.get("pre_start", {})
        post_done = hooks.get("post_done", {})
        commands = executor_config.get("commands", {})
        paths = executor_config.get("paths", {})

        return {
            "max_retries": executor_config.get("max_retries"),
            "retry_delay_seconds": executor_config.get("retry_delay_seconds"),
            "task_timeout_minutes": executor_config.get("task_timeout_minutes"),
            "max_consecutive_failures": executor_config.get("max_consecutive_failures"),
            "claude_command": executor_config.get("claude_command"),
            "claude_model": executor_config.get("claude_model"),
            "skip_permissions": executor_config.get("skip_permissions"),
            "create_git_branch": pre_start.get("create_git_branch"),
            "run_tests_on_done": post_done.get("run_tests"),
            "run_lint_on_done": post_done.get("run_lint"),
            "lint_blocking": post_done.get("lint_blocking"),
            "auto_commit": post_done.get("auto_commit"),
            "test_command": commands.get("test"),
            "lint_command": commands.get("lint"),
            "lint_fix_command": commands.get("lint_fix"),
            "logs_dir": Path(paths["logs"]) if paths.get("logs") else None,
            "state_file": Path(paths["state"]) if paths.get("state") else None,
        }
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: Failed to load config from {config_path}: {e}")
        return {}


def build_config(yaml_config: dict, args: argparse.Namespace) -> ExecutorConfig:
    """Build ExecutorConfig from YAML and CLI arguments.

    CLI arguments override YAML config.

    Args:
        yaml_config: Configuration loaded from YAML file.
        args: Parsed CLI arguments.

    Returns:
        ExecutorConfig instance.
    """
    # Start with defaults
    config_kwargs = {}

    # Apply YAML config (only non-None values)
    for key, value in yaml_config.items():
        if value is not None:
            config_kwargs[key] = value

    # Override with CLI arguments
    if hasattr(args, "max_retries") and args.max_retries != 3:
        config_kwargs["max_retries"] = args.max_retries
    if hasattr(args, "timeout") and args.timeout != 30:
        config_kwargs["task_timeout_minutes"] = args.timeout
    if hasattr(args, "no_tests") and args.no_tests:
        config_kwargs["run_tests_on_done"] = False
    if hasattr(args, "no_branch") and args.no_branch:
        config_kwargs["create_git_branch"] = False
    if hasattr(args, "no_commit") and args.no_commit:
        config_kwargs["auto_commit"] = False

    return ExecutorConfig(**config_kwargs)


# === State Management ===


@dataclass
class TaskAttempt:
    """Task execution attempt"""

    timestamp: str
    success: bool
    duration_seconds: float
    error: str | None = None
    claude_output: str | None = None


@dataclass
class TaskState:
    """Task state in executor"""

    task_id: str
    status: str  # pending, running, success, failed, skipped
    attempts: list = field(default_factory=list)
    started_at: str | None = None
    completed_at: str | None = None

    @property
    def attempt_count(self) -> int:
        return len(self.attempts)

    @property
    def last_error(self) -> str | None:
        if self.attempts:
            return self.attempts[-1].error
        return None


class ExecutorState:
    """Global executor state"""

    def __init__(self, config: ExecutorConfig):
        self.config = config
        self.tasks: dict[str, TaskState] = {}
        self.consecutive_failures = 0
        self.total_completed = 0
        self.total_failed = 0
        self._load()

    def _load(self):
        """Load state from file"""
        if self.config.state_file.exists():
            data = json.loads(self.config.state_file.read_text())
            for task_id, task_data in data.get("tasks", {}).items():
                attempts = [TaskAttempt(**a) for a in task_data.get("attempts", [])]
                self.tasks[task_id] = TaskState(
                    task_id=task_id,
                    status=task_data.get("status", "pending"),
                    attempts=attempts,
                    started_at=task_data.get("started_at"),
                    completed_at=task_data.get("completed_at"),
                )
            self.consecutive_failures = data.get("consecutive_failures", 0)
            self.total_completed = data.get("total_completed", 0)
            self.total_failed = data.get("total_failed", 0)

    def _save(self):
        """Save state to file"""
        self.config.state_file.parent.mkdir(parents=True, exist_ok=True)
        data = {
            "tasks": {
                task_id: {
                    "status": ts.status,
                    "attempts": [
                        {
                            "timestamp": a.timestamp,
                            "success": a.success,
                            "duration_seconds": a.duration_seconds,
                            "error": a.error,
                        }
                        for a in ts.attempts
                    ],
                    "started_at": ts.started_at,
                    "completed_at": ts.completed_at,
                }
                for task_id, ts in self.tasks.items()
            },
            "consecutive_failures": self.consecutive_failures,
            "total_completed": self.total_completed,
            "total_failed": self.total_failed,
            "last_updated": datetime.now().isoformat(),
        }
        self.config.state_file.write_text(json.dumps(data, indent=2))

    def get_task_state(self, task_id: str) -> TaskState:
        if task_id not in self.tasks:
            self.tasks[task_id] = TaskState(task_id=task_id, status="pending")
        return self.tasks[task_id]

    def record_attempt(
        self,
        task_id: str,
        success: bool,
        duration: float,
        error: str | None = None,
        output: str | None = None,
    ):
        """Record execution attempt"""
        state = self.get_task_state(task_id)
        state.attempts.append(
            TaskAttempt(
                timestamp=datetime.now().isoformat(),
                success=success,
                duration_seconds=duration,
                error=error,
                claude_output=output,
            )
        )

        if success:
            state.status = "success"
            state.completed_at = datetime.now().isoformat()
            self.consecutive_failures = 0
            self.total_completed += 1
        else:
            if state.attempt_count >= self.config.max_retries:
                state.status = "failed"
                self.total_failed += 1
            self.consecutive_failures += 1

        self._save()

    def mark_running(self, task_id: str):
        state = self.get_task_state(task_id)
        state.status = "running"
        state.started_at = datetime.now().isoformat()
        self._save()

    def should_stop(self) -> bool:
        """Check if we should stop"""
        return self.consecutive_failures >= self.config.max_consecutive_failures


# === Prompt Builder ===


def extract_test_failures(output: str) -> str:
    """Extract relevant test failure info from pytest output."""
    lines = output.split("\n")
    result_lines = []
    in_failure = False
    failure_count = 0
    max_failures = 5  # Limit to avoid huge prompts

    for line in lines:
        # Capture FAILED lines
        if "FAILED" in line or "ERROR" in line:
            result_lines.append(line)
            failure_count += 1
            if failure_count >= max_failures:
                result_lines.append(f"... and more (showing first {max_failures})")
                break
        # Capture assertion errors
        elif "AssertionError" in line or "assert" in line.lower():
            result_lines.append(line)
        # Capture short summary
        elif "short test summary" in line.lower():
            in_failure = True
        elif in_failure and line.strip():
            result_lines.append(line)

    return "\n".join(result_lines[-50:]) if result_lines else output[-1500:]


def build_task_prompt(
    task: Task,
    config: ExecutorConfig,
    previous_attempts: list[TaskAttempt] | None = None,
) -> str:
    """Build prompt for Claude with task context and previous attempt info."""

    # Read specifications
    spec_dir = config.project_root / "spec"

    requirements = ""
    if (spec_dir / "requirements.md").exists():
        requirements = (spec_dir / "requirements.md").read_text()

    design = ""
    if (spec_dir / "design.md").exists():
        design = (spec_dir / "design.md").read_text()

    # Find related requirements
    related_reqs = []
    for ref in task.traces_to:
        if ref.startswith("REQ-"):
            # Extract requirement from requirements.md
            pattern = rf"#### {ref}:.*?(?=####|\Z)"
            match = re.search(pattern, requirements, re.DOTALL)
            if match:
                related_reqs.append(match.group(0).strip())

    # Find related design
    related_design = []
    for ref in task.traces_to:
        if ref.startswith("DESIGN-"):
            pattern = rf"### {ref}:.*?(?=###|\Z)"
            match = re.search(pattern, design, re.DOTALL)
            if match:
                related_design.append(match.group(0).strip())

    # Checklist
    checklist_text = "\n".join(
        [f"- {'[x]' if done else '[ ]'} {item}" for item, done in task.checklist]
    )

    prompt = f"""# Task Execution Request

## Task: {task.id} ‚Äî {task.name}

**Priority:** {task.priority.upper()}
**Estimate:** {task.estimate}
**Milestone:** {task.milestone}

## Checklist (implement ALL items):

{checklist_text}

## Related Requirements:

{chr(10).join(related_reqs) if related_reqs else "See spec/requirements.md"}

## Related Design:

{chr(10).join(related_design) if related_design else "See spec/design.md"}

## Instructions:

1. Implement ALL checklist items for this task
2. Write unit tests for new code (coverage ‚â•80%)
3. Follow the design patterns from spec/design.md
4. Use existing code style and conventions
5. Create/update files as needed

## Dependencies:

- To add a new dependency: `uv add <package>`
- To add a dev dependency: `uv add --dev <package>`
- NEVER edit pyproject.toml manually for dependencies
- After adding dependencies, they are available immediately

## Success Criteria:

- All checklist items implemented
- All tests pass (`uv run pytest`)
- No lint errors (`uv run ruff check .`)
- Code follows project conventions

## Output:

When complete, respond with:
- Summary of changes made
- Files created/modified
- Any issues or notes
- "TASK_COMPLETE" if successful, or "TASK_FAILED: <reason>" if not

Begin implementation:
"""

    # Add previous attempts context if any failed
    if previous_attempts:
        failed_attempts = [a for a in previous_attempts if not a.success]
        if failed_attempts:
            attempts_section = "\n## ‚ö†Ô∏è PREVIOUS ATTEMPTS FAILED - FIX THESE ISSUES:\n\n"
            for i, attempt in enumerate(failed_attempts, 1):
                attempts_section += f"### Attempt {i} (failed):\n"
                if attempt.error:
                    attempts_section += f"**Error:** {attempt.error}\n\n"
                if attempt.claude_output:
                    # Extract test failures for clarity
                    failures = extract_test_failures(attempt.claude_output)
                    if failures:
                        attempts_section += (
                            f"**Test failures:**\n```\n{failures}\n```\n\n"
                        )

            attempts_section += (
                "**IMPORTANT:** Review the errors above and fix the issues. "
                "Do not repeat the same mistakes.\n\n"
            )

            # Insert before "Begin implementation:"
            prompt = prompt.replace(
                "Begin implementation:", attempts_section + "Begin implementation:"
            )

    return prompt


# === Hooks ===


def get_task_branch_name(task: Task) -> str:
    """Generate branch name for task"""
    safe_name = task.name.lower().replace(" ", "-").replace("/", "-")[:30]
    return f"task/{task.id.lower()}-{safe_name}"


def get_main_branch(config: ExecutorConfig) -> str:
    """Determine main branch name (main or master)"""
    result = subprocess.run(
        ["git", "symbolic-ref", "refs/remotes/origin/HEAD"],
        capture_output=True,
        text=True,
        cwd=config.project_root,
    )
    if result.returncode == 0:
        # refs/remotes/origin/main -> main
        return result.stdout.strip().split("/")[-1]

    # Fallback: check if main or master exists
    for branch in ["main", "master"]:
        result = subprocess.run(
            ["git", "rev-parse", "--verify", branch],
            capture_output=True,
            cwd=config.project_root,
        )
        if result.returncode == 0:
            return branch

    return "main"  # default


def pre_start_hook(task: Task, config: ExecutorConfig) -> bool:
    """Hook before starting task"""
    print(f"üîß Pre-start hook for {task.id}")

    # Sync dependencies
    print("   Syncing dependencies...")
    result = subprocess.run(
        ["uv", "sync"], capture_output=True, text=True, cwd=config.project_root
    )
    if result.returncode == 0:
        print("   ‚úÖ Dependencies synced")
    else:
        print(f"   ‚ö†Ô∏è  uv sync warning: {result.stderr[:200]}")

    # Create git branch
    if config.create_git_branch:
        branch_name = get_task_branch_name(task)
        try:
            # Check if git exists
            result = subprocess.run(
                ["git", "rev-parse", "--git-dir"],
                capture_output=True,
                cwd=config.project_root,
            )
            if result.returncode != 0:
                return True  # No git repository

            # Switch to main
            main_branch = get_main_branch(config)
            subprocess.run(
                ["git", "checkout", main_branch],
                capture_output=True,
                cwd=config.project_root,
            )

            # Check if branch exists
            result = subprocess.run(
                ["git", "rev-parse", "--verify", branch_name],
                capture_output=True,
                cwd=config.project_root,
            )

            if result.returncode == 0:
                # Branch exists ‚Äî switch to it
                subprocess.run(
                    ["git", "checkout", branch_name],
                    capture_output=True,
                    cwd=config.project_root,
                )
                print(f"   Switched to existing branch: {branch_name}")
            else:
                # Create new branch
                result = subprocess.run(
                    ["git", "checkout", "-b", branch_name],
                    capture_output=True,
                    cwd=config.project_root,
                )
                if result.returncode == 0:
                    print(f"   Created branch: {branch_name}")
                else:
                    print(f"   ‚ö†Ô∏è  Failed to create branch: {result.stderr.decode()}")

        except FileNotFoundError:
            pass  # git not installed

    return True


def post_done_hook(
    task: Task, config: ExecutorConfig, success: bool
) -> tuple[bool, str | None]:
    """Hook after task completion.

    Returns:
        Tuple of (success, error_details).
        error_details contains test/lint output on failure.
    """
    print(f"üîß Post-done hook for {task.id} (success={success})")

    if not success:
        return False, None

    # Run tests
    if config.run_tests_on_done:
        print("   Running tests...")
        result = subprocess.run(
            config.test_command,
            shell=True,
            capture_output=True,
            cwd=config.project_root,
        )
        if result.returncode != 0:
            print("   ‚ùå Tests failed!")
            # Combine stdout and stderr for full picture
            test_output = result.stdout.decode() + "\n" + result.stderr.decode()
            print(result.stderr.decode()[:500])
            return False, f"Tests failed:\n{test_output}"
        print("   ‚úÖ Tests passed")

    # Run lint
    if config.run_lint_on_done and config.lint_command:
        print("   Running lint...")
        result = subprocess.run(
            config.lint_command,
            shell=True,
            capture_output=True,
            cwd=config.project_root,
        )

        if result.returncode != 0:
            # Step 1: Attempt auto-fix
            print("   üîß Attempting auto-fix...")
            subprocess.run(
                config.lint_fix_command,
                shell=True,
                capture_output=True,
                cwd=config.project_root,
            )

            # Step 2: Re-check lint
            recheck = subprocess.run(
                config.lint_command,
                shell=True,
                capture_output=True,
                cwd=config.project_root,
            )

            if recheck.returncode != 0:
                # Step 3: Still failing ‚Äî block or warn
                if config.lint_blocking:
                    lint_output = (
                        recheck.stdout.decode() + "\n" + recheck.stderr.decode()
                    )
                    print("   ‚ùå Lint errors remain after auto-fix!")
                    return False, f"Lint errors (not auto-fixable):\n{lint_output}"
                else:
                    print("   ‚ö†Ô∏è  Lint warnings (non-blocking)")
            else:
                print("   ‚úÖ Lint auto-fixed")
        else:
            print("   ‚úÖ Lint passed")

    # Auto-commit
    if config.auto_commit:
        try:
            # Check if there are changes to commit
            status_result = subprocess.run(
                ["git", "status", "--porcelain"],
                capture_output=True,
                text=True,
                cwd=config.project_root,
            )
            if not status_result.stdout.strip():
                print("   No changes to commit")
            else:
                subprocess.run(["git", "add", "-A"], cwd=config.project_root)
                # Build commit message with task details
                commit_title = f"{task.id}: {task.name}"
                commit_body_lines = []
                if task.checklist:
                    commit_body_lines.append("Completed:")
                    for item, checked in task.checklist:
                        if checked:
                            commit_body_lines.append(f"  - {item}")
                if task.milestone:
                    commit_body_lines.append(f"\nMilestone: {task.milestone}")

                commit_msg = commit_title
                if commit_body_lines:
                    commit_msg += "\n\n" + "\n".join(commit_body_lines)

                subprocess.run(
                    ["git", "commit", "-m", commit_msg], cwd=config.project_root
                )
                print("   Committed changes")
        except Exception as e:
            print(f"   Commit failed: {e}")

    # Merge branch to main
    if config.create_git_branch:
        try:
            branch_name = get_task_branch_name(task)
            main_branch = get_main_branch(config)

            # Switch to main
            result = subprocess.run(
                ["git", "checkout", main_branch],
                capture_output=True,
                text=True,
                cwd=config.project_root,
            )
            if result.returncode != 0:
                print(f"   ‚ö†Ô∏è  Failed to switch to {main_branch}")
                return True, None

            # Merge task branch
            result = subprocess.run(
                ["git", "merge", branch_name, "--no-ff", "-m", f"Merge {branch_name}"],
                capture_output=True,
                text=True,
                cwd=config.project_root,
            )
            if result.returncode == 0:
                print(f"   Merged {branch_name} ‚Üí {main_branch}")

                # Delete task branch
                subprocess.run(
                    ["git", "branch", "-d", branch_name],
                    capture_output=True,
                    cwd=config.project_root,
                )
                print(f"   Deleted branch: {branch_name}")
            else:
                print(f"   ‚ö†Ô∏è  Merge failed: {result.stderr}")
                # Return to task branch on failure
                subprocess.run(
                    ["git", "checkout", branch_name],
                    capture_output=True,
                    cwd=config.project_root,
                )
        except Exception as e:
            print(f"   Merge failed: {e}")

    return True, None


# === Task Executor ===


def execute_task(task: Task, config: ExecutorConfig, state: ExecutorState) -> bool:
    """Execute a single task via Claude CLI"""

    task_id = task.id
    print(f"\n{'=' * 60}")
    print(f"üöÄ Executing {task_id}: {task.name}")
    print(f"{'=' * 60}")

    # Pre-start hook
    if not pre_start_hook(task, config):
        print("‚ùå Pre-start hook failed")
        return False

    # Update status
    state.mark_running(task_id)
    update_task_status(TASKS_FILE, task_id, "in_progress")

    # Get previous attempts for context (to inform Claude about past failures)
    task_state = state.get_task_state(task_id)
    previous_attempts = task_state.attempts if task_state.attempts else None

    # Build prompt with previous attempt context
    prompt = build_task_prompt(task, config, previous_attempts)

    # Save prompt to log
    config.logs_dir.mkdir(parents=True, exist_ok=True)
    log_file = (
        config.logs_dir / f"{task_id}-{datetime.now().strftime('%Y%m%d-%H%M%S')}.log"
    )

    with open(log_file, "w") as f:
        f.write(f"=== PROMPT ===\n{prompt}\n\n")

    # Run Claude
    start_time = datetime.now()

    try:
        cmd = [config.claude_command, "-p", prompt]
        if config.skip_permissions:
            cmd.append("--dangerously-skip-permissions")
        if config.claude_model:
            cmd.extend(["--model", config.claude_model])

        flags = " --dangerously-skip-permissions" if config.skip_permissions else ""
        print(f"ü§ñ Running: {config.claude_command} -p ...{flags}")

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=config.task_timeout_minutes * 60,
            cwd=config.project_root,
        )

        duration = (datetime.now() - start_time).total_seconds()
        output = result.stdout

        # Save output
        with open(log_file, "a") as f:
            f.write(f"=== OUTPUT ===\n{output}\n\n")
            f.write(f"=== STDERR ===\n{result.stderr}\n\n")
            f.write(f"=== RETURN CODE: {result.returncode} ===\n")

        # Check result
        # Success if:
        # 1. Explicitly says TASK_COMPLETE, or
        # 2. Return code 0 and no TASK_FAILED (Claude forgot the marker)
        has_complete_marker = "TASK_COMPLETE" in output
        has_failed_marker = "TASK_FAILED" in output
        implicit_success = result.returncode == 0 and not has_failed_marker

        success = (has_complete_marker and not has_failed_marker) or implicit_success

        if success:
            if has_complete_marker:
                print("‚úÖ Claude reports: TASK_COMPLETE")
            else:
                print("‚úÖ Implicit success (return code 0, no TASK_FAILED)")

            # Post-done hook (tests, lint)
            hook_success, hook_error = post_done_hook(task, config, True)

            if hook_success:
                state.record_attempt(task_id, True, duration, output=output)
                update_task_status(TASKS_FILE, task_id, "done")
                mark_all_checklist_done(TASKS_FILE, task_id)
                print(f"‚úÖ {task_id} completed successfully in {duration:.1f}s")
                return True
            else:
                # Hook failed (tests didn't pass)
                # Include detailed error info for next attempt
                error = hook_error or "Post-done hook failed (tests/lint)"
                # Combine Claude output with test failures for context
                full_output = output
                if hook_error:
                    full_output = f"{output}\n\n=== TEST FAILURES ===\n{hook_error}"
                state.record_attempt(
                    task_id, False, duration, error=error, output=full_output
                )
                print(f"‚ùå {task_id} failed: tests/lint check")
                return False
        else:
            # Claude reported failure
            error_match = re.search(r"TASK_FAILED:\s*(.+)", output)
            error = error_match.group(1) if error_match else "Unknown error"
            state.record_attempt(task_id, False, duration, error=error, output=output)
            print(f"‚ùå {task_id} failed: {error}")
            return False

    except subprocess.TimeoutExpired:
        duration = config.task_timeout_minutes * 60
        error = f"Timeout after {config.task_timeout_minutes} minutes"
        state.record_attempt(task_id, False, duration, error=error)
        print(f"‚è∞ {task_id} timed out")
        return False

    except Exception as e:
        duration = (datetime.now() - start_time).total_seconds()
        error = str(e)
        state.record_attempt(task_id, False, duration, error=error)
        print(f"üí• {task_id} error: {error}")
        return False


def run_with_retries(task: Task, config: ExecutorConfig, state: ExecutorState) -> bool:
    """Execute task with retries"""

    task_state = state.get_task_state(task.id)

    for attempt in range(task_state.attempt_count, config.max_retries):
        print(f"\nüìç Attempt {attempt + 1}/{config.max_retries} for {task.id}")

        if execute_task(task, config, state):
            return True

        if attempt < config.max_retries - 1:
            print(f"‚è≥ Waiting {config.retry_delay_seconds}s before retry...")
            import time

            time.sleep(config.retry_delay_seconds)

    print(f"‚ùå {task.id} failed after {config.max_retries} attempts")
    update_task_status(TASKS_FILE, task.id, "blocked")
    return False


# === CLI Commands ===


def cmd_run(args, config: ExecutorConfig):
    """Execute tasks"""

    tasks = parse_tasks(TASKS_FILE)
    state = ExecutorState(config)

    # Check failure limit
    if state.should_stop():
        print(f"‚õî Stopped: {state.consecutive_failures} consecutive failures")
        print("   Use 'executor.py retry <TASK-ID>' to retry specific task")
        return

    # Determine which tasks to execute
    if args.task:
        # Specific task
        task = get_task_by_id(tasks, args.task.upper())
        if not task:
            print(f"‚ùå Task {args.task} not found")
            return
        tasks_to_run = [task]

    elif args.all:
        # All ready tasks
        tasks_to_run = get_next_tasks(tasks)
        if args.milestone:
            tasks_to_run = [
                t for t in tasks_to_run if args.milestone.lower() in t.milestone.lower()
            ]

    elif args.milestone:
        # Tasks for specific milestone
        next_tasks = get_next_tasks(tasks)
        tasks_to_run = [
            t for t in next_tasks if args.milestone.lower() in t.milestone.lower()
        ]

    else:
        # Next task
        next_tasks = get_next_tasks(tasks)
        tasks_to_run = next_tasks[:1] if next_tasks else []

    if not tasks_to_run:
        print("‚úÖ No tasks ready to execute")
        print("   All dependencies might be incomplete, or all tasks done")
        return

    print(f"üìã Tasks to execute: {len(tasks_to_run)}")
    for t in tasks_to_run:
        print(f"   - {t.id}: {t.name}")

    # Execute
    if args.all:
        # For --all mode, continuously re-evaluate ready tasks after each completion
        executed_ids: set[str] = set()
        while True:
            # Re-parse tasks to get updated statuses
            tasks = parse_tasks(TASKS_FILE)
            ready_tasks = get_next_tasks(tasks)

            # Filter by milestone if specified
            if args.milestone:
                ready_tasks = [
                    t
                    for t in ready_tasks
                    if args.milestone.lower() in t.milestone.lower()
                ]

            # Filter out already executed tasks
            ready_tasks = [t for t in ready_tasks if t.id not in executed_ids]

            if not ready_tasks:
                # Show why we're stopping
                all_tasks = parse_tasks(TASKS_FILE)
                todo_tasks = [t for t in all_tasks if t.status == "todo"]
                if todo_tasks:
                    print(f"\n‚è∏Ô∏è  No more ready tasks. {len(todo_tasks)} tasks blocked:")
                    for t in todo_tasks:
                        deps = ", ".join(t.depends_on) if t.depends_on else "none"
                        print(f"   - {t.id}: waiting on [{deps}]")
                else:
                    print("\n‚úÖ All tasks completed!")
                break

            task = ready_tasks[0]
            executed_ids.add(task.id)

            print(f"\nüìã Next ready task: {task.id}: {task.name}")

            success = run_with_retries(task, config, state)

            if not success and state.should_stop():
                print("\n‚õî Stopping: too many consecutive failures")
                break
    else:
        # For single task or milestone mode, execute the fixed list
        for task in tasks_to_run:
            success = run_with_retries(task, config, state)

            if not success and state.should_stop():
                print("\n‚õî Stopping: too many consecutive failures")
                break

    # Summary
    # Re-read tasks to get updated statuses after execution
    tasks = parse_tasks(TASKS_FILE)

    # Calculate statistics
    failed_attempts = sum(
        1 for ts in state.tasks.values() for a in ts.attempts if not a.success
    )
    remaining = len([t for t in tasks if t.status == "todo"])

    print(f"\n{'=' * 60}")
    print("üìä Execution Summary")
    print(f"{'=' * 60}")
    print(f"   Tasks completed:    {state.total_completed}")
    print(f"   Tasks failed:       {state.total_failed}")
    print(f"   Tasks remaining:    {remaining}")
    if failed_attempts > 0:
        print(f"   Failed attempts:    {failed_attempts} (retried successfully)")


def cmd_status(args, config: ExecutorConfig):
    """Execution status"""

    state = ExecutorState(config)

    # Calculate statistics from actual task state
    completed_tasks = sum(1 for ts in state.tasks.values() if ts.status == "success")
    failed_tasks = sum(1 for ts in state.tasks.values() if ts.status == "failed")
    running_tasks = [ts for ts in state.tasks.values() if ts.status == "running"]
    failed_attempts = sum(
        1 for ts in state.tasks.values() for a in ts.attempts if not a.success
    )

    print("\nüìä Executor Status")
    print(f"{'=' * 50}")
    print(f"Tasks completed:       {completed_tasks}")
    print(f"Tasks failed:          {failed_tasks}")
    if running_tasks:
        print(f"Tasks in progress:     {len(running_tasks)}")
    if failed_attempts > 0:
        print(f"Failed attempts:       {failed_attempts} (retried)")
    print(
        f"Consecutive failures:  "
        f"{state.consecutive_failures}/{config.max_consecutive_failures}"
    )

    # Tasks with attempts
    attempted = [ts for ts in state.tasks.values() if ts.attempts]
    if attempted:
        print("\nüìù Task History:")
        for ts in attempted:
            icon = (
                "‚úÖ"
                if ts.status == "success"
                else "‚ùå"
                if ts.status == "failed"
                else "üîÑ"
            )
            attempts_info = f"{ts.attempt_count} attempt"
            if ts.attempt_count > 1:
                attempts_info += "s"
            print(f"   {icon} {ts.task_id}: {ts.status} ({attempts_info})")
            if ts.status == "failed" and ts.last_error:
                print(f"      Last error: {ts.last_error[:50]}...")
            elif ts.status == "running" and ts.last_error:
                print(f"      ‚ö†Ô∏è  Last attempt failed: {ts.last_error[:50]}...")


def cmd_retry(args, config: ExecutorConfig):
    """Retry failed task, preserving error context from previous attempts."""

    tasks = parse_tasks(TASKS_FILE)
    state = ExecutorState(config)

    task = get_task_by_id(tasks, args.task_id.upper())
    if not task:
        print(f"‚ùå Task {args.task_id} not found")
        return

    task_state = state.get_task_state(task.id)

    # Handle --fresh flag
    if hasattr(args, "fresh") and args.fresh:
        print("üßπ Fresh start: clearing previous attempts")
        task_state.attempts = []
    else:
        # Keep previous attempts for context (Claude will see past errors)
        previous_attempts = len(task_state.attempts)
        if previous_attempts > 0:
            print(f"üìã Preserving {previous_attempts} previous attempt(s) for context")
            # Show last error for reference
            if task_state.last_error:
                error_preview = task_state.last_error[:100]
                print(f"   Last error: {error_preview}...")

    # Only reset status and failure counter
    task_state.status = "pending"
    state.consecutive_failures = 0
    state._save()

    print(f"üîÑ Retrying {task.id}...")

    # Execute single attempt (not run_with_retries which has max_retries limit)
    success = execute_task(task, config, state)

    if success:
        update_task_status(TASKS_FILE, task.id, "done")
        mark_all_checklist_done(TASKS_FILE, task.id)
    else:
        update_task_status(TASKS_FILE, task.id, "blocked")


def cmd_logs(args, config: ExecutorConfig):
    """Show task logs"""

    task_id = args.task_id.upper()
    log_files = sorted(config.logs_dir.glob(f"{task_id}-*.log"))

    if not log_files:
        print(f"No logs found for {task_id}")
        return

    latest = log_files[-1]
    print(f"üìÑ Latest log: {latest}")
    print("=" * 50)
    print(latest.read_text()[:5000])  # Limit output


def cmd_reset(args, config: ExecutorConfig):
    """Reset executor state"""

    if config.state_file.exists():
        config.state_file.unlink()
        print("‚úÖ State reset")

    if args.logs and config.logs_dir.exists():
        shutil.rmtree(config.logs_dir)
        print("‚úÖ Logs cleared")


# === Main ===


def main():
    parser = argparse.ArgumentParser(
        description="ATP Task Executor ‚Äî automatic task execution via Claude",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    # Global options
    parser.add_argument(
        "--max-retries", type=int, default=3, help="Max retries per task (default: 3)"
    )
    parser.add_argument(
        "--timeout", type=int, default=30, help="Task timeout in minutes (default: 30)"
    )
    parser.add_argument(
        "--no-tests", action="store_true", help="Skip tests on task completion"
    )
    parser.add_argument(
        "--no-branch", action="store_true", help="Skip git branch creation"
    )
    parser.add_argument(
        "--no-commit", action="store_true", help="Skip auto-commit on success"
    )

    subparsers = parser.add_subparsers(dest="command", help="Commands")

    # run
    run_parser = subparsers.add_parser("run", help="Execute tasks")
    run_parser.add_argument("--task", "-t", help="Specific task ID")
    run_parser.add_argument(
        "--all", "-a", action="store_true", help="Run all ready tasks"
    )
    run_parser.add_argument("--milestone", "-m", help="Filter by milestone")

    # status
    subparsers.add_parser("status", help="Show execution status")

    # retry
    retry_parser = subparsers.add_parser("retry", help="Retry failed task")
    retry_parser.add_argument("task_id", help="Task ID to retry")
    retry_parser.add_argument(
        "--fresh",
        action="store_true",
        help="Clear previous attempts (start fresh, no error context)",
    )

    # logs
    logs_parser = subparsers.add_parser("logs", help="Show task logs")
    logs_parser.add_argument("task_id", help="Task ID")

    # reset
    reset_parser = subparsers.add_parser("reset", help="Reset executor state")
    reset_parser.add_argument("--logs", action="store_true", help="Also clear logs")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    # Load config from YAML file, then override with CLI args
    yaml_config = load_config_from_yaml()
    config = build_config(yaml_config, args)

    # Dispatch
    commands = {
        "run": cmd_run,
        "status": cmd_status,
        "retry": cmd_retry,
        "logs": cmd_logs,
        "reset": cmd_reset,
    }

    cmd_func = commands.get(args.command)
    if cmd_func:
        cmd_func(args, config)


if __name__ == "__main__":
    main()
